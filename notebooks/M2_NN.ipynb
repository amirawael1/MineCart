{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb4f9a2",
   "metadata": {},
   "source": [
    "# QA System Neural Network\n",
    "\n",
    "This notebook implements a Sequence-to-Sequence (Seq2Seq) model for Question Answering using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a6898ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "%pip install torch pandas numpy scikit-learn -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fbd01fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                 context  \\\n",
      "0                                            Item: Christian Religious Rubber Bracelet with Card, 2 1/4 Inch | Cost: 14.99 | Rating: 4.3   \n",
      "1                                            Item: Christian Religious Rubber Bracelet with Card, 2 1/4 Inch | Cost: 14.99 | Rating: 4.3   \n",
      "2                                            Item: Christian Religious Rubber Bracelet with Card, 2 1/4 Inch | Cost: 14.99 | Rating: 4.3   \n",
      "3                                            Item: Christian Religious Rubber Bracelet with Card, 2 1/4 Inch | Cost: 14.99 | Rating: 4.3   \n",
      "4  Item: 2 Pcs Mickey Ears, Minnie Costume Ears Headband for Children Mom Baby Boys Girls Women Party (RED 2) | Cost: 8.79 | Rating: 4.5   \n",
      "\n",
      "                    question  \\\n",
      "0      What's the price tag?   \n",
      "1        What is the rating?   \n",
      "2       Name of the product?   \n",
      "3  Is it a top selling item?   \n",
      "4     How much does it cost?   \n",
      "\n",
      "                                                      answer        type  \n",
      "0                                                      14.99       price  \n",
      "1                                                  4.3 stars      rating  \n",
      "2  Christian Religious Rubber Bracelet with Card, 2 1/4 Inch       title  \n",
      "3                                                         No  bestseller  \n",
      "4                                                       8.79       price  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/processed/qa_dataset.csv')\n",
    "\n",
    "# Display the head of the dataframe\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e4e0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 169717\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        # Special tokens\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3, \"<SEP>\": 4}\n",
    "        self.idx2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\", 4: \"<SEP>\"}\n",
    "        self.vocab_size = 5\n",
    "        \n",
    "    def fit_on_text(self, text_list):\n",
    "        # Build vocabulary from list of texts\n",
    "        all_words = []\n",
    "        for text in text_list:\n",
    "            # Simple preprocessing: lowercase and split\n",
    "            words = str(text).lower().split()\n",
    "            all_words.extend(words)\n",
    "            \n",
    "        counter = collections.Counter(all_words)\n",
    "        \n",
    "        # Add words to vocabulary\n",
    "        for word, count in counter.items():\n",
    "            if word not in self.word2idx:\n",
    "                self.word2idx[word] = self.vocab_size\n",
    "                self.idx2word[self.vocab_size] = word\n",
    "                self.vocab_size += 1\n",
    "                \n",
    "    def encode(self, text):\n",
    "        # Convert text to list of indices\n",
    "        words = str(text).lower().split()\n",
    "        return [self.word2idx.get(w, self.word2idx[\"<UNK>\"]) for w in words]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        # Convert list of indices back to text\n",
    "        return ' '.join([self.idx2word.get(idx, \"<UNK>\") for idx in indices])\n",
    "\n",
    "# Initialize and train tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# Use lowercase column names to match the CSV file\n",
    "all_text = df['context'].tolist() + df['question'].tolist() + df['answer'].tolist()\n",
    "tokenizer.fit_on_text(all_text)\n",
    "print(f\"Vocabulary Size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f98a1ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_idx = self.tokenizer.word2idx[\"<SEP>\"]\n",
    "        self.sos_idx = self.tokenizer.word2idx[\"<SOS>\"]\n",
    "        self.eos_idx = self.tokenizer.word2idx[\"<EOS>\"]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Use lowercase column names\n",
    "        context = row['context']\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        \n",
    "        # Encode inputs\n",
    "        context_idx = self.tokenizer.encode(context)\n",
    "        question_idx = self.tokenizer.encode(question)\n",
    "        \n",
    "        # Input Tensor: Context + <SEP> + Question\n",
    "        input_indices = context_idx + [self.sep_idx] + question_idx\n",
    "        \n",
    "        # Target Tensor: <SOS> + Answer + <EOS>\n",
    "        answer_idx = self.tokenizer.encode(answer)\n",
    "        target_indices = [self.sos_idx] + answer_idx + [self.eos_idx]\n",
    "        \n",
    "        return torch.tensor(input_indices, dtype=torch.long), torch.tensor(target_indices, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    pad_idx = tokenizer.word2idx[\"<PAD>\"]\n",
    "    \n",
    "    # Pad sequences to the same length within the batch\n",
    "    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=pad_idx)\n",
    "    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=pad_idx)\n",
    "    \n",
    "    return inputs_padded, targets_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cbc5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # input: [batch_size, 1]\n",
    "        output = self.embedding(input)\n",
    "        output = self.dropout(output)\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        prediction = self.out(output)\n",
    "        return prediction, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: [batch_size, src_len]\n",
    "        # trg: [batch_size, trg_len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.out.out_features\n",
    "        \n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # Last hidden state of the encoder is the context\n",
    "        _, hidden = self.encoder(src)\n",
    "        \n",
    "        # First input to the decoder is the <SOS> token\n",
    "        input = trg[:, 0].unsqueeze(1)\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[:, t, :] = output.squeeze(1)\n",
    "            \n",
    "            # Get the highest predicted token\n",
    "            top1 = output.argmax(2) \n",
    "            \n",
    "            # Teacher forcing: decide if next input is from target or prediction\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            input = trg[:, t].unsqueeze(1) if teacher_force else top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f884ceca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: 131,301,877\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "INPUT_DIM = tokenizer.vocab_size\n",
    "OUTPUT_DIM = tokenizer.vocab_size\n",
    "HIDDEN_DIM = 256\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# Model Initialization\n",
    "enc = EncoderRNN(INPUT_DIM, HIDDEN_DIM, ENC_DROPOUT).to(device)\n",
    "dec = DecoderRNN(HIDDEN_DIM, OUTPUT_DIM, DEC_DROPOUT).to(device)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "# Optimization\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.word2idx[\"<PAD>\"])\n",
    "\n",
    "# Data Preparation\n",
    "train_df, val_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "train_dataset = QADataset(train_df, tokenizer)\n",
    "val_dataset = QADataset(val_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57669421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        # output: [batch size, trg len, output dim]\n",
    "        # trg: [batch size, trg len]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        # Flatten outputs and targets for loss calculation\n",
    "        # Skip the first token (<SOS>)\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (src, trg) in enumerate(iterator):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            \n",
    "            output = model(src, trg, 0) # Turn off teacher forcing\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, 1)\n",
    "    val_loss = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. PPL: {math.exp(val_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cc491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(model, context, question, max_len=50):\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare input\n",
    "    c_idx = tokenizer.encode(context)\n",
    "    q_idx = tokenizer.encode(question)\n",
    "    s_idx = [tokenizer.word2idx[\"<SEP>\"]]\n",
    "    input_indices = c_idx + s_idx + q_idx\n",
    "    \n",
    "    src_tensor = torch.tensor(input_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "        \n",
    "    # Start decoding with SOS\n",
    "    trg_indexes = [tokenizer.word2idx[\"<SOS>\"]]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        trg_tensor = trg_tensor.unsqueeze(0) # [1, 1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(trg_tensor, hidden)\n",
    "            \n",
    "        pred_token = output.argmax(2).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        \n",
    "        if pred_token == tokenizer.word2idx[\"<EOS>\"]:\n",
    "            break\n",
    "            \n",
    "    # Decode indices to text\n",
    "    predicted_tokens = trg_indexes[1:] # remove SOS\n",
    "    \n",
    "    # Remove EOS if present\n",
    "    if predicted_tokens and predicted_tokens[-1] == tokenizer.word2idx[\"<EOS>\"]:\n",
    "         predicted_tokens = predicted_tokens[:-1]\n",
    "         \n",
    "    return tokenizer.decode(predicted_tokens)\n",
    "\n",
    "# Evaluation on a few validation samples\n",
    "print(\"--- Inference Examples ---\")\n",
    "for i in range(3):\n",
    "    sample = val_df.iloc[i]\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"Context: {sample['context'][:100]}...\")\n",
    "    print(f\"Question: {sample['question']}\")\n",
    "    print(f\"True Answer: {sample['answer']}\")\n",
    "    prediction = predict_answer(model, sample['context'], sample['question'])\n",
    "    print(f\"Predicted Answer: {prediction}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6221bb",
   "metadata": {},
   "source": [
    "# Interactive Chat with History\n",
    "Test the QA system with conversation history management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467060f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat History Management\n",
    "class ChatSession:\n",
    "    def __init__(self, model, tokenizer, device, context):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.context = context\n",
    "        self.history = []  # List of (question, answer) tuples\n",
    "        \n",
    "    def ask(self, question):\n",
    "        # Build input with history\n",
    "        history_str = \"\"\n",
    "        for q, a in self.history[-3:]:  # Keep last 3 turns for context\n",
    "            history_str += f\" Q: {q} A: {a}\"\n",
    "        \n",
    "        # Combine: Context + History + Current Question\n",
    "        full_context = f\"{self.context}{history_str}\"\n",
    "        \n",
    "        # Get prediction\n",
    "        answer = predict_answer(self.model, full_context, question)\n",
    "        \n",
    "        # Update history\n",
    "        self.history.append((question, answer))\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def reset(self):\n",
    "        self.history = []\n",
    "\n",
    "# Demo: Interactive session with a sample product\n",
    "sample_product = val_df.iloc[0]['context']\n",
    "print(f\"Product Context: {sample_product}\\n\")\n",
    "\n",
    "session = ChatSession(model, tokenizer, device, sample_product)\n",
    "\n",
    "# Simulate a multi-turn conversation\n",
    "demo_questions = [\n",
    "    \"What is the price?\",\n",
    "    \"How many stars does it have?\",\n",
    "    \"Is this a best seller?\"\n",
    "]\n",
    "\n",
    "print(\"--- Multi-turn Conversation Demo ---\")\n",
    "for q in demo_questions:\n",
    "    answer = session.ask(q)\n",
    "    print(f\"User: {q}\")\n",
    "    print(f\"Bot: {answer}\")\n",
    "    print()\n",
    "\n",
    "# To enable true interactivity, uncomment below:\n",
    "# while True:\n",
    "#     q = input(\"You: \")\n",
    "#     if q.lower() == 'exit': break\n",
    "#     print(f\"Bot: {session.ask(q)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
